<!DOCTYPE html>
<html lang="en-us"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
   <meta name="description" content="And why you likely shouldn&rsquo;t
Intro
I&rsquo;ve been craving more parallel compute power in my home recently, and have been looking around for
a long time on something that can fit my student budget/work with my present hardware. Specifically
i had a old motherboard and 8th-gen Intel CPU laying around that i wanted to make use of. A few
alternatives i considered:


The boring standard answer of buying a used RTX 3090 which seems to be still the best tradeoff on
price/power/flops/memory for most people. But buying a 600â‚¬ card for a 100â‚¬ platform felt like a
bit of an overkill, especially since my windows PC that i use for mostly gaming is only running a
old GTX 1660.">  

  <title>
    
      Running an old AMD Datacenter card at home
    
  </title>


  <link rel="shortcut icon" type="image/x-icon" href="/" />
  
  
  
  <link rel="stylesheet" href="/css/main.5a9ec16bd5a18572ff3b3a27cf1e01000a4c7bf880d3aafbfa52f4597e753ead91fd383ca50f83a20b5975ee90e61532d2faae267e6aee86491dcb17485af1e7.css" integrity="sha512-Wp7Ba9WhhXL/Ozonzx4BAApMe/iA06r7&#43;lL0WX51Pq2R/Tg8pQ&#43;DogtZde6Q5hUy0vquJn5q7oZJHcsXSFrx5w==" />
  
  <link rel="stylesheet" href="/syntax.css"/>
</head>
<body a="auto">
        <main class="page-content" aria-label="Content">
            <div class="w">
                <div class="post-meta">
                    <a href="/">--&gt;</a>

                    <p>
                        <time datetime="2025-07-14 00:00:00 &#43;0000 UTC">
                            2025-07-14
                        </time>
                    </p>
                </div>

<article>
    <h1>Running an old AMD Datacenter card at home</h1>

    

    <p>And why <em>you</em> likely shouldn&rsquo;t</p>
<h2 id="intro">Intro</h2>
<p>I&rsquo;ve been craving more parallel compute power in my home recently, and have been looking around for
a long time on something that can fit my student budget/work with my present hardware. Specifically
i had a old motherboard and 8th-gen Intel CPU laying around that i wanted to make use of. A few
alternatives i considered:</p>
<ul>
<li>
<p>The boring standard answer of buying a used RTX 3090 which seems to be still the best tradeoff on
price/power/flops/memory for most people. But buying a 600â‚¬ card for a 100â‚¬ platform felt like a
bit of an overkill, especially since my windows PC that i use for mostly gaming is only running a
old GTX 1660.</p>
</li>
<li>
<p>AMD and Intel have been putting out some pretty interesting <em>new</em> GPUs like the 9070/9060 or the
Arc B580s, and likely this would&rsquo;ve been the more mature and easy solution, but something that
irked me about this specifically the memory-bandwith to price ratio, since i was planing to run
LLMs on it.</p>
</li>
<li>
<p>The last one that im still considering, once my budget allows it, are the Tenstorrent cards
(Wormhole/Blackhole), since they seem to not only be technicalogically interesting but also have
some good power-efficiency from what i hear. But 1kâ‚¬ is just too much for me currently, so this
sadly has to wait (unless someone has a Grayskull card laying around that they want to get rid of).</p>
</li>
</ul>
<hr>
<p>So instead of choosing any of these sane, well-prove solutions, i decided to buy myself some problems.
Years ago i noticed that old NVIDIA Tesla HPC cards (V100 etc.) are going for quite cheap on eBay, and i got
reminded of that now recently when i saw someone on r/localllama talking about using a bunch of AMD Instinct Mi25
for at home inference (also this <a href="https://forum.level1techs.com/t/mi25-stable-diffusions-100-hidden-beast/194172">L1 forum post</a>).</p>
<p>So i looked on eBay and found a seller right here in Germany selling Refurbed Mi50s for ~110â‚¬ a piece, which on one evening
around mightnight (best time for decisions obviously) decided to order, alongside an improved PSU since the card is sometimes
reported to have a power-limit of 300 watts. 3 days later and i received a beautiful, outdated, and shiny new toy:</p>
<figure><img src="/posts/mi50/card.jpeg"
    alt="Image of the card" width="70%">
</figure>

<p>Some interesting things to note about these cards:</p>
<ul>
<li>It sports 16GB of HBM2 memory at a reported ~ 1TB/s of bandwith  (which for that price has to be a contender for cheapest GB/s/â‚¬ ratio)</li>
<li>60 Compute-units (CUs) AMDs equivalent of the SMs with a wavefront/warpsize of 64 (<em>not the usual 32</em>), though ive read somewhere that this is configurable</li>
<li>only ~27 TFLOP/s of FP16 ðŸ˜¿ (and afaik no support for any smaller datatypes, so no Q4/Q8 quantizations)</li>
</ul>
<p>So while its definitely an interesting card, there&rsquo;s a reason they&rsquo;re so cheap to get&hellip; primarily that horrendous TFLOPS/W ratio.</p>
<h2 id="setup-problems">Setup problems</h2>
<p>I&rsquo;ve had this card for close to a week now and had quite a few hurdles to cross until i got it into a somewhat operating state.</p>
<ul>
<li>As mentioned i had to buy a PSU that supported the 300Watt via 2 8-pin PCI cables, but that wouldve likely been a given with any of the alternatives</li>
<li>The card is built for a blowthrough-style server chasis, so i had to MacGyver myself a fan shroud/duct on the end of it</li>
</ul>
<p><figure><img src="/posts/mi50/installed.jpeg"
    alt="Image of the system with the pink 3D-printed fan-duct" width="70%">
</figure>

I still haven&rsquo;t  fully load tested the card, so im unsure if this arrangment will be enough to cool it. But sofar its holding the GPU at &lt;45 celsius.</p>
<p>I thought with this i might have halve of the work done, but it turned out the hardware side was alot more straightforward to solve, compared to the
hustle the software turned out to be (and still is). For documentations sake (if anyone else should be naive enough to try and set these up) i will quickly
detail my issues and how i solved/circumvented them:</p>
<ol>
<li>While some docs talk of howto setup the drivers for the card on Arch, i mostly found Ubuntu related docs, specifically Ubuntu 22.04 LTS. I initially tried 24.04, but ditched it since
the offical docs for the ROCm version i chose only talk about Ubuntu 20.04 and 22.04.</li>
<li>Enable &lsquo;Above 4G decoding&rsquo; in the BIOS settings, which as-far as i understand it enables PCIe devices to use larger then 32-bit virtual addresses or something, but im not quite sure. This is definitely required for the card to run though!</li>
<li>I had to twiddle around with my linux-kernel cmdline arguments quite a bit, the final solution that worked for me is <code>pci_realloc pci=realloc,hpmemsize=512M,hpiosize=32M</code> but dont ask me to explain this to you, as i dont fully understand them either.</li>
<li>The Vega 20/GFX-9 ISA got dropped from the offical ROCm support list, so i chose a older ROCm release for my setup (5.6.0) as the newest one i tried (6.4.0) didn&rsquo;t end up working</li>
</ol>
<p>After all this the card finally got recognized by the amdgpu-drivers, but while trying to build the HIP-Basics from <a href="https://github.com/ROCm/rocm-examples">ROCm-examples</a> i also had to install
the libstdc++-12-dev package to fix the <code>fatal error: 'cmath' file not found</code> linker errors. Finally one should also not forget to add their own user to the <code>render</code> group, since otherwise the
user-space programs (like <code>rocm-smi</code>) aren&rsquo;t allowed to interface with the card and dont report it (this is totally not an issue that bugged me for an hour&hellip;).</p>
<h2 id="results-">Results.. ?</h2>
<p>But with all this done i can finally run some code on it, in this case ive mostly focused on getting the HIP-basics running and then some of my tinygrad based ML projects.</p>
<p>For example the basic mat-mul kernel from the HIP-Basics:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fish" data-lang="fish"><span class="line"><span class="cl"><span class="nf">jakob</span>@jupiter ~/g/r/H/matrix_multiplication<span class="o">&gt;</span> make <span class="o">&amp;&amp;</span> ./hip_matrix_multiplication
</span></span><span class="line"><span class="cl">/opt/rocm/bin/hipcc <span class="na">-std</span><span class="o">=</span>c++<span class="m">17</span> <span class="na">-Wall</span> <span class="na">-Wextra</span> <span class="na">-I</span> ../../Common <span class="na">-I</span> ../../External  <span class="na">-o</span> hip_matrix_multiplication main.hip
</span></span><span class="line"><span class="cl"><span class="nf">starting</span> calculation
</span></span><span class="line"><span class="cl"><span class="nf">Matrix</span> multiplication: <span class="o">[</span>8192x8192<span class="o">]</span> * <span class="o">[</span>8192x8192<span class="o">]</span>, block size: 16x16
</span></span><span class="line"><span class="cl"><span class="nf">Duration</span>: <span class="m">472</span>.<span class="m">72</span> ms
</span></span><span class="line"><span class="cl"><span class="nf">GFLOPs</span>: <span class="m">2325</span>.<span class="m">95</span>
</span></span></code></pre></div><p><em>NOTE: The mat-mul kernel is relatively basic and not optimized, with a more tuned kernel i would expect almost 10x of that FLOPs number.</em></p>
<p>Or i can run the tinygrad examples:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fish" data-lang="fish"><span class="line"><span class="cl"><span class="nf">jakob</span>@jupiter ~/g/t/examples <span class="o">(</span><span class="nf">master</span><span class="o">)&gt;</span> <span class="nv">GPU</span><span class="o">=</span><span class="m">1</span> <span class="nf">uv</span> run beautiful_mnist.py
</span></span><span class="line"><span class="cl"><span class="nf">loss</span>:   <span class="m">0</span>.<span class="m">09</span> test_accuracy: <span class="m">98</span>.<span class="m">19</span>%: <span class="m">100</span>%<span class="o">|</span>â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ<span class="o">|</span> <span class="nf">70</span>/<span class="m">70</span> <span class="o">[</span><span class="m">00</span>:<span class="m">09</span><span class="o">&lt;</span><span class="m">00</span>:<span class="m">00</span>,  <span class="m">7</span>.01it/s<span class="o">]</span>
</span></span></code></pre></div><p>This uses the OpenCL backend if im not quite wrong, i know tinygrad has a RX7900 specific backend or something close to that, but for now i think we are going via OpenCL.</p>
<p>I might write another blogpost about the results of me learning HIP and understanding the architecture a little better, but for now im relatively happy i got it all working at all.</p>
<h2 id="tldr">tl;dr</h2>
<p>Should you buy yourself an Instinct Mi50 (or any of these older HPC cards for that matter)?</p>
<p>Likely not (with a caveat).</p>
<p>A definite no-no if you want to make money with it, since the power usage almost immediatly invalidates its existence, especially with the power-costs here in Germany.
It&rsquo;s also not super helpful if you want to just use LLMs since i have yet to find an inference-tool that works on the GPU, and since it doesn&rsquo;t support anything smaller then FP16 you
likely wont find a good application there for it either.</p>
<p>I would say really the only category of people that should buy this are people that are already familar with GPUs (but maybe not AMD GPUs) or just want to tinker around a little with out
throwing out too much money immediatley. It&rsquo;s faster for ML-training then my M2 Macbook Air and also then my GTX 1660 via WSL, so thats something. But i wont likely leave it running over night,
especially since if im desperate i can still use my uni&rsquo;s HPC-cluster.</p>

</article>

            </div>
        </main>
    </body></html>
